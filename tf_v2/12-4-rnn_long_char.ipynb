{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMm79EXSTk8qmdTklYIEXEg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"qHPpnRaLN8Rm","executionInfo":{"status":"ok","timestamp":1721114471451,"user_tz":-540,"elapsed":7107,"user":{"displayName":"신명근","userId":"14329502532076519745"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"code","source":["sentence = (\"if you want to build a ship, don't drum up people together to \"\n","            \"collect wood and don't assign them tasks and work, but rather \"\n","            \"teach them to long for the endless immensity of the sea.\")"],"metadata":{"id":"NqEQPWHoONY0","executionInfo":{"status":"ok","timestamp":1721114471451,"user_tz":-540,"elapsed":5,"user":{"displayName":"신명근","userId":"14329502532076519745"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["char_set = list(set(sentence))\n","char_dic = {w: i for i, w in enumerate(char_set)}"],"metadata":{"id":"TboCHMwtOeNY","executionInfo":{"status":"ok","timestamp":1721114471451,"user_tz":-540,"elapsed":4,"user":{"displayName":"신명근","userId":"14329502532076519745"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data_dim = len(char_set)\n","hidden_size = len(char_set)\n","num_classes = len(char_set)\n","sequence_length = 10 # Any arbitrary number\n","learning_rate = 0.1"],"metadata":{"id":"dL8r4Oe_Om-P","executionInfo":{"status":"ok","timestamp":1721114471451,"user_tz":-540,"elapsed":4,"user":{"displayName":"신명근","userId":"14329502532076519745"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["dataX = []\n","dataY = []\n","\n","for i in range(0, len(sentence) - sequence_length):\n","  x_str = sentence[i:i + sequence_length]\n","  y_str = sentence[i + 1:i + sequence_length + 1]\n","  print(i, x_str, '->', y_str)\n","\n","  x = [char_dic[c] for c in x_str] # x str to index\n","  y = [char_dic[c] for c in y_str] # y str to index\n","\n","  dataX.append(x)\n","  dataY.append(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofnTu1UDO1O7","executionInfo":{"status":"ok","timestamp":1721114471451,"user_tz":-540,"elapsed":4,"user":{"displayName":"신명근","userId":"14329502532076519745"}},"outputId":"4c1278b1-60a4-42b1-f582-95f370465be4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0 if you wan -> f you want\n","1 f you want ->  you want \n","2  you want  -> you want t\n","3 you want t -> ou want to\n","4 ou want to -> u want to \n","5 u want to  ->  want to b\n","6  want to b -> want to bu\n","7 want to bu -> ant to bui\n","8 ant to bui -> nt to buil\n","9 nt to buil -> t to build\n","10 t to build ->  to build \n","11  to build  -> to build a\n","12 to build a -> o build a \n","13 o build a  ->  build a s\n","14  build a s -> build a sh\n","15 build a sh -> uild a shi\n","16 uild a shi -> ild a ship\n","17 ild a ship -> ld a ship,\n","18 ld a ship, -> d a ship, \n","19 d a ship,  ->  a ship, d\n","20  a ship, d -> a ship, do\n","21 a ship, do ->  ship, don\n","22  ship, don -> ship, don'\n","23 ship, don' -> hip, don't\n","24 hip, don't -> ip, don't \n","25 ip, don't  -> p, don't d\n","26 p, don't d -> , don't dr\n","27 , don't dr ->  don't dru\n","28  don't dru -> don't drum\n","29 don't drum -> on't drum \n","30 on't drum  -> n't drum u\n","31 n't drum u -> 't drum up\n","32 't drum up -> t drum up \n","33 t drum up  ->  drum up p\n","34  drum up p -> drum up pe\n","35 drum up pe -> rum up peo\n","36 rum up peo -> um up peop\n","37 um up peop -> m up peopl\n","38 m up peopl ->  up people\n","39  up people -> up people \n","40 up people  -> p people t\n","41 p people t ->  people to\n","42  people to -> people tog\n","43 people tog -> eople toge\n","44 eople toge -> ople toget\n","45 ople toget -> ple togeth\n","46 ple togeth -> le togethe\n","47 le togethe -> e together\n","48 e together ->  together \n","49  together  -> together t\n","50 together t -> ogether to\n","51 ogether to -> gether to \n","52 gether to  -> ether to c\n","53 ether to c -> ther to co\n","54 ther to co -> her to col\n","55 her to col -> er to coll\n","56 er to coll -> r to colle\n","57 r to colle ->  to collec\n","58  to collec -> to collect\n","59 to collect -> o collect \n","60 o collect  ->  collect w\n","61  collect w -> collect wo\n","62 collect wo -> ollect woo\n","63 ollect woo -> llect wood\n","64 llect wood -> lect wood \n","65 lect wood  -> ect wood a\n","66 ect wood a -> ct wood an\n","67 ct wood an -> t wood and\n","68 t wood and ->  wood and \n","69  wood and  -> wood and d\n","70 wood and d -> ood and do\n","71 ood and do -> od and don\n","72 od and don -> d and don'\n","73 d and don' ->  and don't\n","74  and don't -> and don't \n","75 and don't  -> nd don't a\n","76 nd don't a -> d don't as\n","77 d don't as ->  don't ass\n","78  don't ass -> don't assi\n","79 don't assi -> on't assig\n","80 on't assig -> n't assign\n","81 n't assign -> 't assign \n","82 't assign  -> t assign t\n","83 t assign t ->  assign th\n","84  assign th -> assign the\n","85 assign the -> ssign them\n","86 ssign them -> sign them \n","87 sign them  -> ign them t\n","88 ign them t -> gn them ta\n","89 gn them ta -> n them tas\n","90 n them tas ->  them task\n","91  them task -> them tasks\n","92 them tasks -> hem tasks \n","93 hem tasks  -> em tasks a\n","94 em tasks a -> m tasks an\n","95 m tasks an ->  tasks and\n","96  tasks and -> tasks and \n","97 tasks and  -> asks and w\n","98 asks and w -> sks and wo\n","99 sks and wo -> ks and wor\n","100 ks and wor -> s and work\n","101 s and work ->  and work,\n","102  and work, -> and work, \n","103 and work,  -> nd work, b\n","104 nd work, b -> d work, bu\n","105 d work, bu ->  work, but\n","106  work, but -> work, but \n","107 work, but  -> ork, but r\n","108 ork, but r -> rk, but ra\n","109 rk, but ra -> k, but rat\n","110 k, but rat -> , but rath\n","111 , but rath ->  but rathe\n","112  but rathe -> but rather\n","113 but rather -> ut rather \n","114 ut rather  -> t rather t\n","115 t rather t ->  rather te\n","116  rather te -> rather tea\n","117 rather tea -> ather teac\n","118 ather teac -> ther teach\n","119 ther teach -> her teach \n","120 her teach  -> er teach t\n","121 er teach t -> r teach th\n","122 r teach th ->  teach the\n","123  teach the -> teach them\n","124 teach them -> each them \n","125 each them  -> ach them t\n","126 ach them t -> ch them to\n","127 ch them to -> h them to \n","128 h them to  ->  them to l\n","129  them to l -> them to lo\n","130 them to lo -> hem to lon\n","131 hem to lon -> em to long\n","132 em to long -> m to long \n","133 m to long  ->  to long f\n","134  to long f -> to long fo\n","135 to long fo -> o long for\n","136 o long for ->  long for \n","137  long for  -> long for t\n","138 long for t -> ong for th\n","139 ong for th -> ng for the\n","140 ng for the -> g for the \n","141 g for the  ->  for the e\n","142  for the e -> for the en\n","143 for the en -> or the end\n","144 or the end -> r the endl\n","145 r the endl ->  the endle\n","146  the endle -> the endles\n","147 the endles -> he endless\n","148 he endless -> e endless \n","149 e endless  ->  endless i\n","150  endless i -> endless im\n","151 endless im -> ndless imm\n","152 ndless imm -> dless imme\n","153 dless imme -> less immen\n","154 less immen -> ess immens\n","155 ess immens -> ss immensi\n","156 ss immensi -> s immensit\n","157 s immensit ->  immensity\n","158  immensity -> immensity \n","159 immensity  -> mmensity o\n","160 mmensity o -> mensity of\n","161 mensity of -> ensity of \n","162 ensity of  -> nsity of t\n","163 nsity of t -> sity of th\n","164 sity of th -> ity of the\n","165 ity of the -> ty of the \n","166 ty of the  -> y of the s\n","167 y of the s ->  of the se\n","168  of the se -> of the sea\n","169 of the sea -> f the sea.\n"]}]},{"cell_type":"code","source":["batch_size = len(dataX)"],"metadata":{"id":"KdSnF9eyPXZm","executionInfo":{"status":"ok","timestamp":1721114471452,"user_tz":-540,"elapsed":3,"user":{"displayName":"신명근","userId":"14329502532076519745"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# One-hot encoding\n","X_one_hot = tf.one_hot(dataX, num_classes)\n","Y_one_hot = tf.one_hot(dataY, num_classes)\n","\n","print(X_one_hot.shape) # check out the shape (170, 10, 25)\n","print(Y_one_hot.shape) # check out the shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7R7qmX4hPZc9","executionInfo":{"status":"ok","timestamp":1721114471983,"user_tz":-540,"elapsed":534,"user":{"displayName":"신명근","userId":"14329502532076519745"}},"outputId":"88527deb-dab7-48ab-84f9-ae1451d7a2af"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(170, 10, 25)\n","(170, 10, 25)\n"]}]},{"cell_type":"code","source":["tf.model = tf.keras.Sequential()\n","tf.model.add(tf.keras.layers.LSTM(units = num_classes,\n","                                  input_shape = (sequence_length, X_one_hot.shape[2]), return_sequences = True))\n","tf.model.add(tf.keras.layers.LSTM(units = num_classes, return_sequences = True))\n","tf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units = num_classes, activation = 'softmax')))\n","tf.model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMIGlptePr6M","executionInfo":{"status":"ok","timestamp":1721114473665,"user_tz":-540,"elapsed":1683,"user":{"displayName":"신명근","userId":"14329502532076519745"}},"outputId":"05a15aae-31c1-46eb-d820-541c299e4545"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 10, 25)            5100      \n","                                                                 \n"," lstm_1 (LSTM)               (None, 10, 25)            5100      \n","                                                                 \n"," time_distributed (TimeDist  (None, 10, 25)            650       \n"," ributed)                                                        \n","                                                                 \n","=================================================================\n","Total params: 10850 (42.38 KB)\n","Trainable params: 10850 (42.38 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["tf.model.compile(loss = 'categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(lr = learning_rate),\n","                 metrics = ['accuracy'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vz83X-HNQQze","executionInfo":{"status":"ok","timestamp":1721114473665,"user_tz":-540,"elapsed":4,"user":{"displayName":"신명근","userId":"14329502532076519745"}},"outputId":"f17103bd-ca36-443d-eab4-58a3d83b60d0"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]}]},{"cell_type":"code","source":["tf.model.fit(X_one_hot, Y_one_hot, epochs = 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aep3ZrASQZXl","executionInfo":{"status":"ok","timestamp":1721114485959,"user_tz":-540,"elapsed":12298,"user":{"displayName":"신명근","userId":"14329502532076519745"}},"outputId":"0f7a044d-6c0b-4286-fae3-8e2e5edf9c9d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","6/6 [==============================] - 7s 14ms/step - loss: 3.2131 - accuracy: 0.0871\n","Epoch 2/100\n","6/6 [==============================] - 0s 12ms/step - loss: 3.2007 - accuracy: 0.1665\n","Epoch 3/100\n","6/6 [==============================] - 0s 9ms/step - loss: 3.1854 - accuracy: 0.1865\n","Epoch 4/100\n","6/6 [==============================] - 0s 10ms/step - loss: 3.1646 - accuracy: 0.1876\n","Epoch 5/100\n","6/6 [==============================] - 0s 10ms/step - loss: 3.1342 - accuracy: 0.1888\n","Epoch 6/100\n","6/6 [==============================] - 0s 10ms/step - loss: 3.0876 - accuracy: 0.1894\n","Epoch 7/100\n","6/6 [==============================] - 0s 10ms/step - loss: 3.0347 - accuracy: 0.1894\n","Epoch 8/100\n","6/6 [==============================] - 0s 9ms/step - loss: 2.9955 - accuracy: 0.1894\n","Epoch 9/100\n","6/6 [==============================] - 0s 10ms/step - loss: 2.9675 - accuracy: 0.1894\n","Epoch 10/100\n","6/6 [==============================] - 0s 10ms/step - loss: 2.9423 - accuracy: 0.1894\n","Epoch 11/100\n","6/6 [==============================] - 0s 10ms/step - loss: 2.9269 - accuracy: 0.1894\n","Epoch 12/100\n","6/6 [==============================] - 0s 12ms/step - loss: 2.9149 - accuracy: 0.1894\n","Epoch 13/100\n","6/6 [==============================] - 0s 13ms/step - loss: 2.9031 - accuracy: 0.1894\n","Epoch 14/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.8920 - accuracy: 0.1894\n","Epoch 15/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.8819 - accuracy: 0.1894\n","Epoch 16/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.8715 - accuracy: 0.1894\n","Epoch 17/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.8615 - accuracy: 0.1894\n","Epoch 18/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.8523 - accuracy: 0.1894\n","Epoch 19/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.8429 - accuracy: 0.1894\n","Epoch 20/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.8341 - accuracy: 0.1894\n","Epoch 21/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.8248 - accuracy: 0.1894\n","Epoch 22/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.8153 - accuracy: 0.1894\n","Epoch 23/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.8058 - accuracy: 0.1894\n","Epoch 24/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.7958 - accuracy: 0.1888\n","Epoch 25/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.7853 - accuracy: 0.1888\n","Epoch 26/100\n","6/6 [==============================] - 0s 6ms/step - loss: 2.7740 - accuracy: 0.1888\n","Epoch 27/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.7613 - accuracy: 0.1900\n","Epoch 28/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.7485 - accuracy: 0.1918\n","Epoch 29/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.7351 - accuracy: 0.1912\n","Epoch 30/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.7201 - accuracy: 0.1941\n","Epoch 31/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.7053 - accuracy: 0.1941\n","Epoch 32/100\n","6/6 [==============================] - 0s 9ms/step - loss: 2.6901 - accuracy: 0.2059\n","Epoch 33/100\n","6/6 [==============================] - 0s 10ms/step - loss: 2.6746 - accuracy: 0.2024\n","Epoch 34/100\n","6/6 [==============================] - 0s 10ms/step - loss: 2.6608 - accuracy: 0.2112\n","Epoch 35/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.6444 - accuracy: 0.2171\n","Epoch 36/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.6296 - accuracy: 0.2235\n","Epoch 37/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.6148 - accuracy: 0.2253\n","Epoch 38/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.6025 - accuracy: 0.2229\n","Epoch 39/100\n","6/6 [==============================] - 0s 6ms/step - loss: 2.5860 - accuracy: 0.2400\n","Epoch 40/100\n","6/6 [==============================] - 0s 6ms/step - loss: 2.5680 - accuracy: 0.2359\n","Epoch 41/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.5520 - accuracy: 0.2547\n","Epoch 42/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.5371 - accuracy: 0.2676\n","Epoch 43/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.5195 - accuracy: 0.2665\n","Epoch 44/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.5027 - accuracy: 0.2753\n","Epoch 45/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.4858 - accuracy: 0.2841\n","Epoch 46/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.4678 - accuracy: 0.2929\n","Epoch 47/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.4491 - accuracy: 0.2994\n","Epoch 48/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.4341 - accuracy: 0.3012\n","Epoch 49/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.4161 - accuracy: 0.3118\n","Epoch 50/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.4010 - accuracy: 0.3047\n","Epoch 51/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.3844 - accuracy: 0.3194\n","Epoch 52/100\n","6/6 [==============================] - 0s 9ms/step - loss: 2.3657 - accuracy: 0.3165\n","Epoch 53/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.3503 - accuracy: 0.3312\n","Epoch 54/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.3341 - accuracy: 0.3347\n","Epoch 55/100\n","6/6 [==============================] - 0s 9ms/step - loss: 2.3200 - accuracy: 0.3324\n","Epoch 56/100\n","6/6 [==============================] - 0s 9ms/step - loss: 2.3043 - accuracy: 0.3471\n","Epoch 57/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.2878 - accuracy: 0.3471\n","Epoch 58/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.2724 - accuracy: 0.3553\n","Epoch 59/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.2581 - accuracy: 0.3512\n","Epoch 60/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.2409 - accuracy: 0.3571\n","Epoch 61/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.2220 - accuracy: 0.3588\n","Epoch 62/100\n","6/6 [==============================] - 0s 6ms/step - loss: 2.2086 - accuracy: 0.3659\n","Epoch 63/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.1956 - accuracy: 0.3747\n","Epoch 64/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.1844 - accuracy: 0.3724\n","Epoch 65/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.1669 - accuracy: 0.3771\n","Epoch 66/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.1502 - accuracy: 0.3835\n","Epoch 67/100\n","6/6 [==============================] - 0s 6ms/step - loss: 2.1347 - accuracy: 0.3871\n","Epoch 68/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.1201 - accuracy: 0.3912\n","Epoch 69/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.1045 - accuracy: 0.3935\n","Epoch 70/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.0906 - accuracy: 0.3982\n","Epoch 71/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.0758 - accuracy: 0.4006\n","Epoch 72/100\n","6/6 [==============================] - 0s 9ms/step - loss: 2.0611 - accuracy: 0.4059\n","Epoch 73/100\n","6/6 [==============================] - 0s 9ms/step - loss: 2.0475 - accuracy: 0.4141\n","Epoch 74/100\n","6/6 [==============================] - 0s 9ms/step - loss: 2.0347 - accuracy: 0.4141\n","Epoch 75/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.0213 - accuracy: 0.4182\n","Epoch 76/100\n","6/6 [==============================] - 0s 7ms/step - loss: 2.0067 - accuracy: 0.4235\n","Epoch 77/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.9932 - accuracy: 0.4265\n","Epoch 78/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.9807 - accuracy: 0.4324\n","Epoch 79/100\n","6/6 [==============================] - 0s 8ms/step - loss: 1.9673 - accuracy: 0.4359\n","Epoch 80/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.9532 - accuracy: 0.4365\n","Epoch 81/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.9409 - accuracy: 0.4376\n","Epoch 82/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.9266 - accuracy: 0.4435\n","Epoch 83/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.9135 - accuracy: 0.4518\n","Epoch 84/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.8991 - accuracy: 0.4547\n","Epoch 85/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.8849 - accuracy: 0.4582\n","Epoch 86/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.8734 - accuracy: 0.4641\n","Epoch 87/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.8604 - accuracy: 0.4624\n","Epoch 88/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.8489 - accuracy: 0.4706\n","Epoch 89/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.8340 - accuracy: 0.4659\n","Epoch 90/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.8208 - accuracy: 0.4712\n","Epoch 91/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.8070 - accuracy: 0.4788\n","Epoch 92/100\n","6/6 [==============================] - 0s 8ms/step - loss: 1.7948 - accuracy: 0.4735\n","Epoch 93/100\n","6/6 [==============================] - 0s 9ms/step - loss: 1.7823 - accuracy: 0.4829\n","Epoch 94/100\n","6/6 [==============================] - 0s 9ms/step - loss: 1.7698 - accuracy: 0.4806\n","Epoch 95/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.7562 - accuracy: 0.4853\n","Epoch 96/100\n","6/6 [==============================] - 0s 8ms/step - loss: 1.7445 - accuracy: 0.4906\n","Epoch 97/100\n","6/6 [==============================] - 0s 8ms/step - loss: 1.7324 - accuracy: 0.4982\n","Epoch 98/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.7194 - accuracy: 0.4988\n","Epoch 99/100\n","6/6 [==============================] - 0s 8ms/step - loss: 1.7066 - accuracy: 0.5006\n","Epoch 100/100\n","6/6 [==============================] - 0s 7ms/step - loss: 1.6944 - accuracy: 0.5047\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x79f7821d76a0>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["results = tf.model.predict(X_one_hot)\n","\n","for j, result in enumerate(results):\n","  index = np.argmax(result, axis = 1)\n","  if j == 0: # print all for the first result to make a sentence\n","    print(''.join([char_set[t] for t in index]), end = '')\n","  else:\n","    print(char_set[index[-1]], end = '')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"go9a8B_KQdmK","executionInfo":{"status":"ok","timestamp":1721114487518,"user_tz":-540,"elapsed":1564,"user":{"displayName":"신명근","userId":"14329502532076519745"}},"outputId":"8a23a1e4-2808-4c3d-bd79-923e609de9bb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["6/6 [==============================] - 1s 3ms/step\n","d to  tot  to auuld t tssi  aonn  auu  up ppplle tol the  th tolle   toon an  aonn  assiii the  toss  ann aonn  uuu uat e  thte ethe  toodonl ton the ttole ssiimetsiim to the tenn"]}]}]}